import streamlit as st
import google.generativeai as genai
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
import os


# --- 1. åˆæœŸè¨­å®šã¨UI ---
st.set_page_config(page_title="ç¥æˆ¸å¤§å­¦å·¥å­¦éƒ¨ å­¦ç”Ÿä¾¿è¦§ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ", page_icon="ğŸ“š")
st.title("ğŸ“š ç¥æˆ¸å¤§å­¦å·¥å­¦éƒ¨ å­¦ç”Ÿä¾¿è¦§ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
st.caption("2024å¹´åº¦ã®å­¦ç”Ÿä¾¿è¦§ã‚’ã‚‚ã¨ã«ã€AIãŒè³ªå•ã«å›ç­”ã—ã¾ã™ã€‚")

# --- 2. APIã‚­ãƒ¼ã®è¨­å®š ---
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except (FileNotFoundError, KeyError):
    GOOGLE_API_KEY = st.sidebar.text_input("Google AI API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", type="password")

if not GOOGLE_API_KEY:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Google AI APIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
try:
    genai.configure(api_key=GOOGLE_API_KEY)
except Exception as e:
    st.error(f"Google APIã‚­ãƒ¼ã®è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    st.stop()

# --- 3. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰ ---
@st.cache_resource
def load_and_build_vector_store():
    try:
        with open("kobe_u_handbook.txt", "r", encoding="utf-8") as f:
            raw_text = f.read()
    except FileNotFoundError:
        st.error("ã‚¨ãƒ©ãƒ¼: `kobe_u_handbook.txt` ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1500,
        chunk_overlap=200,
        length_function=len,
    )
    text_chunks = text_splitter.split_text(raw_text)
    
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)  # â† ã“ã“ã‚‚ä¿®æ­£
        return vector_store
    except Exception as e:
        st.error(f"ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚APIã‚­ãƒ¼ã‚„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®äº’æ›æ€§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")
        return None


vector_store = load_and_build_vector_store()
if vector_store is None:
    st.stop()

# --- 4. å›ç­”ç”ŸæˆAIãƒã‚§ãƒ¼ãƒ³ã®æº–å‚™ ---
def get_conversational_chain():
    prompt_template = """
    ã‚ãªãŸã¯ç¥æˆ¸å¤§å­¦å·¥å­¦éƒ¨ã®å­¦ç”Ÿä¾¿è¦§ã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã‚‹ã€è¦ªåˆ‡ã§å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    ä»¥ä¸‹ã®ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€æƒ…å ±ã¨ã€Œãƒãƒ£ãƒƒãƒˆå±¥æ­´ã€ã‚’å…ƒã«ã€ã€Œè³ªå•ã€ã«å¯¾ã—ã¦æ—¥æœ¬èªã§è©³ã—ãã€ä¸å¯§ã«å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
    å›ç­”ã¯ã€å­¦ç”Ÿã«ã¨ã£ã¦åˆ†ã‹ã‚Šã‚„ã™ã„ã‚ˆã†ã«ã€å¿…è¦ã§ã‚ã‚Œã°ç®‡æ¡æ›¸ããªã©ã‚’ç”¨ã„ã¦æ•´ç†ã—ã¦ãã ã•ã„ã€‚
    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç­”ãˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ç„¡ç†ã«ç­”ãˆã‚’ä½œæˆã›ãšã€ã€Œå­¦ç”Ÿä¾¿è¦§ã®æƒ…å ±ã‹ã‚‰ã¯å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€ã¨æ˜ç¢ºã«ä¼ãˆã¦ãã ã•ã„ã€‚
    å¿…ãšã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã«åŸºã¥ã„ã¦å›ç­”ã—ã€ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§ç­”ãˆãªã„ã§ãã ã•ã„ã€‚

    ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€‘
    {context}

    ã€ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã€‘
    {chat_history}

    ã€è³ªå•ã€‘
    {question}

    ã€å›ç­”ã€‘
    """
    model = ChatGoogleGenerativeAI(model="gemini-pro", temperature=0.3)
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "chat_history", "question"])
    chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
    return chain

# --- 5. ãƒãƒ£ãƒƒãƒˆUIã®å®Ÿè£… ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ï¼ç¥æˆ¸å¤§å­¦å·¥å­¦éƒ¨ã®å­¦ç”Ÿä¾¿è¦§ã«ã¤ã„ã¦ã€ä½•ã§ã‚‚è³ªå•ã—ã¦ãã ã•ã„ã€‚"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_question := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...ï¼ˆä¾‹: æ—©æœŸå’æ¥­ã®æ¡ä»¶ã¯ï¼Ÿï¼‰"):
    st.session_state.messages.append({"role": "user", "content": user_question})
    with st.chat_message("user"):
        st.markdown(user_question)

    with st.chat_message("assistant"):
        with st.spinner("AIãŒå­¦ç”Ÿä¾¿è¦§ã‚’ç¢ºèªã—ã¦ã„ã¾ã™..."):
            try:
                retriever = vector_store.as_retriever(search_kwargs={"k": 5})
                docs = retriever.invoke(user_question)

                chat_history = "\n".join([f"{'Q' if msg['role'] == 'user' else 'A'}: {msg['content']}" for msg in st.session_state.messages[:-1]])
                chain = get_conversational_chain()
                response = chain(
                    {"input_documents": docs, "chat_history": chat_history, "question": user_question},
                    return_only_outputs=True
                )
                answer = response["output_text"]

                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

# --- 6. æ³¨æ„äº‹é … ---
st.sidebar.markdown("---")
st.sidebar.info(
    "**ã€æ³¨æ„äº‹é …ã€‘**\n"
    "ã“ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ã€2024å¹´åº¦ã®å­¦ç”Ÿä¾¿è¦§ã®æƒ…å ±ã‚’åŸºã«AIãŒå›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
    "å›ç­”ã¯å¿…ãšã—ã‚‚100%æ­£ç¢ºãƒ»å®Œå…¨ã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼ã™ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
    "æœ€çµ‚çš„ãªç¢ºèªã¯ã€å¿…ãšå…¬å¼ã®å­¦ç”Ÿä¾¿è¦§ã§è¡Œã£ã¦ãã ã•ã„ã€‚"
)
